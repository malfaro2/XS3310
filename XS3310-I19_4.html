<!DOCTYPE html>
<html>
  <head>
    <title>XS3310 Teoría Estadística</title>
    <meta charset="utf-8">
    <meta name="author" content="Prof. Marcela Alfaro Córdoba" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# XS3310 Teoría Estadística
## I Semestre 2019
### Prof. Marcela Alfaro Córdoba
### 3/05/2018 (updated: 2019-03-21)

---






# ¿Qué vamos a discutir hoy?

* Propiedades de los estimadores: eficiente, consistente y suficiente.

---

# Eficiencia
	
La eficiencia de un estimador está relacionada a la variabilidad de dicho estimador, la cual se ve representada por el error cuadrático medio. 
	
Ejemplo: Sea `\(X_{1}, X_{2}, ... , X_{n}\)` una muestra aleatoria tal que 
	
`\(F_{X}(x) = \begin{cases} 0 \quad si \quad x &lt; 0 \\ \left(\frac{x}{\theta}\right)^{5} \quad si \quad 0 \leq x \leq \theta \\ 1 \quad si \quad x &gt; \theta \end{cases}\)`
	
i. Sean `\(\overline{X}\)` y `\(X_{(n)}\)` dos estimadores de `\(\theta\)`. ¿Cuál de ellos tiene el menor error cuadrático medio? 

Solución: Podemos reconocer que `\(X_{j} \sim Potencial\left(5,\theta\right)\)`. Sabemos que para una variable aleatoria `\(Y\)` tal que `\(Y \sim Potencial\left(\alpha, \beta\right)\)` su valor esperado y variancia vienen dados por
		
`\(E(Y) = \frac{\alpha\beta}{\alpha+1} \qquad Var(Y) = \frac{\alpha \beta^{2}}{\left(\alpha+1\right)^{2}\left(\alpha+2\right)}\)`
		

---

# Eficiencia
	
Empezando por `\(\overline{X}\)` tenemos lo siguiente,
		
`\(E(\overline{X}) = E(X) = \frac{5\theta}{6}\)`
		
`\(\Rightarrow B(\overline{X}) = \frac{5\theta}{6} - \theta = \frac{-\theta}{6}\)`
		
`\(Var(\overline{X}) = \frac{Var(X)}{n} = \frac{5\theta^{2}}{252n}\)`
		
`\(\Rightarrow ECM(\overline{X}) = Var(\overline{X}) + B(\overline{X})^{2} = \frac{5\theta^{2}}{252n} + \frac{\theta^2}{36} = \frac{\theta^{2}\left(5+7n\right)}{252n}\)`
		
Procedemos a obtener el error cuadrático medio de `\(X_{(n)}\)` pero, primero hay que encontrar cómo se distribuye este estimador,
		
`\(F_{X_{\left(n\right)}}\left(x\right) = \left[F_{X}\left(x\right)\right]^{n} = \left[\left(\frac{x}{\theta}\right)^{5}\right]^{n} = \left(\frac{x}{\theta}\right)^{5n}\)`
		
		
---

# Eficiencia

Podemos reconocer con esto que `\(X_{(n)} \sim Potencial\left(5n,\theta\right)\)`. Ya con esto podemos proceder a obtener la información que necesitamos para el error cuadrático medio:
		
`$$E(X_{(n)}) = \frac{5n\theta}{5n+1}$$`
		
`\(\Rightarrow B(X_{(n)}) = \frac{5n\theta}{5n+1} - \theta = \frac{-\theta}{5n+1}\)`
		
`\(Var(X_{(n)}) = \frac{5n\theta^2}{\left(5n+1\right)^{2}\left(5n+2\right)}\)`
		
`\(\Rightarrow ECM(X_{(n)}) = \frac{5n\theta^2}{\left(5n+1\right)^{2}\left(5n+2\right)} + \frac{\theta^2}{\left(5n+1\right)^{2}} = \frac{10n\theta^{2} + 2\theta^{2}}{\left(5n+1\right)^{2}\left(5n+2\right)} = \frac{2\theta^2}{\left(5n+1\right)\left(5n+2\right)}\)`
		
Observando ambos resultados concluimos que `\(ECM(\overline{X}) &gt; ECM(X_{(n)}), \quad \forall n &gt; 1\)`.
		
---

# Eficiencia

ii. Encontrar dos estimadores insesgados, `\(\hat{\theta}_{1}\)` y `\(\hat{\theta}_{2}\)`, a partir de `\(\overline{X}\)` y `\(X_{(n)}\)` respectivamente. ¿Cuál de ellos tiene menor variabilidad? 

Solución: Podemos notar que los estimadores insesgados serían los siguientes:
		
`\(\hat{\theta}_{1} = \frac{6\overline{X}}{5} \qquad \qquad \hat{\theta}_{2} = \frac{5n+1}{5n}X_{(n)}\)`
		
`\(Var(\hat{\theta}_{1}) = Var\left(\frac{6\overline{X}}{5}\right) = \frac{36}{25}Var(\overline{X}) = \frac{36}{25} \cdot \frac{5\theta^{2}}{252n} = \frac{\theta^2}{35n}\)`
		
`\(Var(\hat{\theta}_{2}) = Var\left(\frac{5n+1}{5n}X_{(n)}\right) = \frac{\left(5n+1\right)^2}{25n^2}Var(X_{(n)})= \frac{\left(5n+1\right)^2}{25n^2} \cdot \frac{5n\theta^2}{\left(5n+1\right)^{2}\left(5n+2\right)} = \frac{\theta^2}{5n\left(5n+2\right)}\)`

En este caso `\(\frac{\theta^2}{5n\left(5n+2\right)} &lt; \frac{\theta^2}{35n} \forall n &gt; 1\)`. 
		
Se dice que `\(\hat{\theta}_{2}\)` es relativamente más eficiente que `\(\hat{\theta}_{1}\)`.

---

# Eficiencia

En general, si se tienen dos estimadores insesgados de un parámetro `\(\theta\)`, es más eficiente aquel que tiene menor variancia. Si se comparan estimadores sesgados se comparan los errores cuadráticos medios en lugar de las variancias. 
	
&gt; Definicion 1.9. Eficiencia relativa. Si `\(\hat{\theta}_{1}\)` y `\(\hat{\theta}_{2}\)` son estimadores insesgados de un parámetro `\(\theta\)`, se define la *eficiencia relativa* de `\(\hat{\theta}_{1}\)` con respecto a `\(\hat{\theta}_{2}\)`: 
  `\(eff(\hat{\theta}_{1},\hat{\theta}_{2}) = \frac{Var(\hat{\theta}_{2})}{Var(\hat{\theta}_{1})}\)`

En el ejemplo anterior:
	
`\(eff(\hat{\theta}_{1},\hat{\theta}_{2}) = \frac{\frac{\theta^2}{5n\left(5n+2\right)}}{\frac{\theta^2}{35n}} = \frac{7}{5n+2} &lt; 1 \quad \forall n&gt;1\)`
	
Esto significa que `\(\hat{\theta}_{2}\)` es más eficiente que `\(\hat{\theta}_{1}\)`.

---

# Eficiencia
	
Podemos apreciar la eficiencia relativa de estos dos estimadores en la Figura 2. Los puntos negros vacios representan a `\(\hat{\theta}_1\)` mientras que los puntos azules rellenos representan a `\(\hat{\theta}_2\)`. Podemos observar como estos segundos tienen puntos en general más próximos al verdadero valor de `\(\theta\)` que los primeros, por lo tanto son relativamente más eficientes. 
![Figura 3.](./eficienciarelativa.jpg)


---

# Eficiencia

Para buscar la eficiencia al estimar un parámetro se requiere determinar el estimador insesgado `\(\hat{\theta}\)` que tiene variancia mínima. 

&gt; Teorema 1.22. Desigualdad de Cramer-Rao. Sea `\(X_{1}, X_{2}, ... , X_{n}\)` una muestra aleatoria sobre una población con parámetro desconocido `\(\theta\)` con función de densidad `\(f_{X}(x|\theta)\)`, cuyo dominio no depende de `\(\theta\)`. Si `\(\hat{\theta}\)` es un estimador insesgado para `\(\theta\)`, entonces en general: `\(Var(\hat{\theta}) \geq I(\theta)\)` donde `\(I(\theta)\)` se llama la **información de Fisher** y se define como `\(I(\theta) = \left[nE\left(-\frac{\partial^{2}\ln(f_{X}(x))}{\partial \theta^{2}}\right)\right]^{-1}\)`. Si `\(Var(\hat{\theta}) = I(\theta)\)` entonces `\(\hat{\theta}\)` es un estimador insesgado de variancia mínima para `\(\theta\)`. 

---

# Eficiencia
	
Ejemplo: Sea `\(X_{1}, X_{2}, ... , X_{n}\)` una muestra aleatoria de una población Normal con media `\(\mu\)` y variancia `\(\sigma^{2}\)`. Demuestre que `\(\overline{X}\)` es un estimador insesgado de variancia mínima (EIVM) para `\(\mu\)`.
	
Solución: Ya sabemos de ejemplos anteriores que `\(\overline{X}\)` es insesgado para `\(\mu\)`. Hay que demostrar que es de variancia mínima utilizando la desigualdad de Cramer-Rao. 
	
Sabemos que en este caso `\(f_{X}(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\)`.
	
`\(\Rightarrow \ln(f_{X}(x)) = \ln\left(\frac{1}{\sqrt{2\pi}\sigma}\right) - \frac{(x-\mu)^2}{2\sigma^2} \Rightarrow \frac{\partial\ln(f_{X}(x))}{\partial\mu} = \frac{x-\mu}{\sigma^{2}}\)`
	
`\(\Rightarrow \frac{\partial^{2}\ln(f_{X}(x))}{\partial\mu^{2}} = \frac{-1}{\sigma^2} \Rightarrow -\frac{\partial^{2}\ln(f_{X}(x))}{\partial\mu^{2}} = \frac{1}{\sigma^2} \Rightarrow I(\mu) =  \left[nE\left(-\frac{\partial^{2}\ln(f_{X}(x))}{\partial \mu^{2}}\right)\right]^{-1} = \left[ \frac{n}{\sigma^2}\right]^{-1}  = \frac{\sigma^2}{n}\)` 
	
De antemano ya sabiamos que `\(Var(\overline{X}) = \frac{\sigma^2}{n}\)` que como vemos es igual a la información de Fisher para `\(\mu\)`, por lo que podemos concluir que `\(\overline{X}\)` es un estimador insesgado de variancia mínima para `\(\mu\)`. 

---

# Consistencia
	
Ejemplo. Sea `\(X_{1}, X_{2}, ... , X_{n}\)` una muestra aleatoria tal que `\(X_{j} \sim Unif(0,\theta)\)` y sean `\(\overline{X}\)` y `\(X_{(n)}\)` estimadores de `\(\theta\)`. 
	
Podemos encontrar que `\(E(\overline{X}) = \frac{\theta}{2}\)` y `\(E(X_{(n)}) = \frac{n\theta}{n+1}\)`, por lo que `\(\overline{X}\)` y `\(X_{(n)}\)` son estimadores sesgados. No obstante podemos construir estimadores insesgados a partir de ellos que queden de la forma `\(\hat{\theta}_{1} = 2\overline{X}\)` y `\(\hat{\theta}_{2} = \frac{n+1}{n}X_{(n)}\)`, respectivamente. 
	
También podemos encontrar que la variancias de estos dos estimadores son las siguientes:
	
`\(Var(\hat{\theta}_{1}) = \frac{\theta^2}{3n} \qquad Var(\hat{\theta}_{2}) = \frac{\theta^2}{n(n+2)}\)`
	
El interés ahora yace en ver si estos estimadores se aproximan cada vez más al valor verdadero de `\(\theta\)` conforme aumentamos el tamaño de muestra y obtenemos más información. Y si ambos cumplen esto también nos podría interesar cuál lo hace de manera más rápida. 

---

# Consistencia

Haciendo una simulación, con `\(\theta\)` = 5, podemos observar que ambos estimadores convergen a `\(\theta\)` conforme el tamaño de muestra va aumentando. Podemos decir que `\(\hat{\theta}_{1}\)` y `\(\hat{\theta}_{2}\)` son estimadores consistentes para estimar `\(\theta\)`. 
	
![Figura 1. Consistencia para dos estimadores insesgados de `\(\theta\)`](./consistencia.jpg)

---

# Consistencia

&gt; Definición 1.10. Consistencia. Se dice que un estimador `\(\hat{\theta}\)` es consistente para estimar `\(\theta\)` ( `\(\hat{\theta}\)` converge en probabilidad a `\(\theta\)` ) si `\(\forall \varepsilon&gt;0\)` se cumple que: `\(\lim_{n \to +\infty}P\left(\left|\hat{\theta}-\theta\right|\leq \varepsilon\right) = 1\)`
		
O lo que es equivalente: `\(\lim_{n \to +\infty} P\left(\left|\hat{\theta}-\theta\right|&gt; \varepsilon\right) = 0\)`

&gt; Teorema 1.23. Si `\(\hat{\theta}\)` es un estimador de `\(\theta\)` entonces `\(\hat{\theta}\)` es un estimador consistente si:

&gt; a. `\(\hat{\theta}\)` es insesgado para `\(\theta\)` 

&gt; b. `\(\lim_{n \to +\infty} Var(\hat{\theta}) = 0\)`

---

# Consistencia

Ejemplo. Sea `\(X_{1}, X_{2}, ... , X_{n}\)` una muestra aleatoria tal que `\(X_{j} \sim Unif(0,\theta)\)` y sean `\(\hat{\theta}_{1} = 2\overline{X}\)` y `\(\hat{\theta}_{2} = \frac{n+1}{n}X_{(n)}\)` dos estimadores de `\(\theta\)`. Demuestre que estos estimadores son consistentes para estimar `\(\theta\)`.
	
Solución. Ya sabemos que `\(\hat{\theta}_{1}\)` y `\(\hat{\theta}_{2}\)` son estimadores insesgados de `\(\theta\)`, por lo que se cumple la primera condición del Teorema 3.1; solo nos hace falta demostrar la segunda propiedad:
	
`\(\lim_{n \to +\infty}Var(\hat{\theta}_{1}) = \lim_{n \to +\infty}\frac{\theta^2}{3n} = 0\)`
	
`\(\lim_{n \to +\infty}Var(\hat{\theta}_{2}) = \lim_{n \to +\infty}\frac{\theta^2}{n(n+2)} = 0\)`
	
Como ambos estimadores cumplen ser insesgados y tener variancias que tienden a cero conforme el tamaño de muestra tiende a infinito entonces se cumple que ambos son estimadores consistentes para `\(\theta\)`. 

---

# Consistencia

No obstante, si observamos el gráfico de la consistencia de `\(X_{(n)}\)` podemos notar como este también tiene un comportamiento consistente pero sin ser un estimador insesgado. Esto significa que la Definición 1.10 abarca muchos más estimadores que el Teorema 1.23. 
![Figura 2. Consistencia para el máximo de una `\(Unif(0,\theta)\)`](./max_consist.jpg)
---

# Consistencia

A continuación demostraremos que `\(X_{(n)}\)` es consistente para `\(\theta\)` por medio de la definición.

Ejemplo. Sean `\(X_{1}, X_{2}, ... , X_{n}\)` una muestra aleatoria tal que `\(X_{j} \sim Unif(0,\theta)\)` y `\(X_{(n)}\)` un estimador de `\(\theta\)`. Demuestre que `\(X_{(n)}\)` es consistente para estimar `\(\theta\)`.
	
Solución. Como `\(X_{(n)}\)` es un estimador sesgado para `\(\theta\)` no podemos hacer uso del Teorema 3.1 por lo que debemos usar la definición de consistencia. Como esta nos pide encontrar probabilidades y sabemos que `\(X_{(n)}\sim Potencial(n,\theta)\)` vamos a recordar la función de distribución de `\(X_{(n)}\)`:
	
`\(F_{X_{(n)}}(x) = \begin{cases} 0\quad si \quad x \leq 0 \\ \left(\frac{x}{\theta}\right)^{n} \quad si\quad 0&lt;x&lt;\theta \\ 1\quad si\quad x \geq \theta \end{cases}\)`
---

# Consistencia

Con esto podemos desarrollar la probabilidad en la definición:
	
`\(P\left(\left|X_{(n)}-\theta\right|\leq \varepsilon\right) = P\left(-\varepsilon \leq X_{(n)}-\theta\leq \varepsilon\right) = P\left(\theta - \varepsilon \leq X_{(n)} \leq \theta + \varepsilon\right) =\)` 
`\(F_{X_{(n)}}(\theta + \varepsilon) - F_{X_{(n)}}(\theta - \varepsilon)\)`

Sabemos que `\(F_{X_{(n)}}(\theta + \varepsilon) = 1 \quad \forall \varepsilon &gt; 0\)` ya que `\(\theta + \varepsilon &gt; \theta\)`. No obstante, dependiendo del valor de `\(\varepsilon\)` puede que `\(\theta - \varepsilon\)` sea menor a 0 o esté entre 0 y `\(\theta\)`, por lo que  `\(F_{X_{(n)}}(\theta - \varepsilon)\)` puede tomar distintos valores dependiendo de `\(\varepsilon\)`:

`\(F_{X_{(n)}}(\theta - \varepsilon) = \begin{cases} \left(\frac{\theta - \varepsilon}{\theta}\right)^{n} \quad si \quad 0 &lt; \varepsilon &lt; \theta \\ 0 \quad si \quad \varepsilon \geq \theta \end{cases}\)`
	
Por lo tanto, aplicando la definición, obtenemos:
	
`\(\lim_{n \to +\infty} P\left(\left|X_{(n)}-\theta\right|\leq \varepsilon\right) = \lim_{n \to +\infty}\left(F_{X_{(n)}}(\theta + \varepsilon) - F_{X_{(n)}}(\theta - \varepsilon)\right) = 1 - \lim_{n \to +\infty}F_{X_{(n)}}(\theta - \varepsilon)\)`
	
`\(=\begin{cases} 1 - \lim_{n \to +\infty}\left(\frac{\theta - \varepsilon}{\theta}\right)^{n} \quad si \quad 0 &lt; \varepsilon &lt; \theta \\ 1 \quad si \quad \varepsilon \geq \theta \end{cases} = 1 \quad \forall \varepsilon &gt; 0\)`
---

# Consistencia

Como se cumple la definición entonces concluimos que `\(X_{(n)}\)` es un estimador consistente para `\(\theta\)`.
	
&gt; Teorema 1.24. Suponga que `\(\hat{\theta}\)` es un estimador consistente para estimar `\(\theta\)` y que `\(\hat{\phi}\)` es un estimador consistente para `\(\phi\)`, entonces:

&gt; `\(\hat{\theta} \pm \hat{\phi}\)` es consistente para estimar `\(\theta \pm \phi\)`

&gt; `\(\hat{\theta} \cdot \hat{\phi}\)` es consistente para estimar `\(\theta \cdot \phi\)`

&gt; `\(\frac{\hat{\theta}}{\hat{\phi}}\)` es consistente para estimar `\(\frac{\theta}{\phi}\)`

&gt; Si `\(g(\cdot)\)` es una función continua en `\(\theta\)` entonces `\(g(\hat{\theta})\)` es consistente para estimar `\(g(\theta)\)`

---

# Consistencia

Ejemplo. Sea `\(X_{1}, X_{2}, ... , X_{n}\)` una muestra aleatoria de una población Normal con media `\(\mu\)` y variancia `\(\sigma^2\)`. 
	
a. Demuestre que `\(S^2\)` es un estimador consistente para `\(\sigma^2\)`. 

Solución: Ya conocemos que `\(E(S^2) = \sigma^2\)` por lo que se cumple que `\(S^2\)` es insesgado para `\(\sigma^2\)`. También sabemos que `\(Var(S^2) = \frac{2\sigma^4}{n-1}\)`, por lo tanto se cumple que `\(\lim_{n \to +\infty}Var(S^2) = 0\)`. Por lo tanto `\(S^2\)` es un estimador consistente para `\(\sigma^2\)`. 

b. Pruebe que `\(S\)` es un estimador consistente para estimar `\(\sigma\)`. 

Solución: Sea `\(g(x) = \sqrt{x}\)` una función continua si `\(x \geq 0\)`, por lo tanto: `\(g(S^2) = S\)` es consistente para estimar `\(g(\sigma^2) = \sigma\)`	

---

# Consistencia

&gt; Teorema 1.25. Teorema de Slutsky. Suponga que `\(U_n\)` es una variable aleatoria que tiene distribución que converge a una `\(N(0,1)\)` cuando `\(n \to +\infty\)`. Además `\(W_n\)` es una variable aleatoria que converge en probabilidad a 1. Se cumple, entonces, que la variable aleatoria `\(\frac{U_n}{W_n}\)` tiene distribución que converge a una `\(N(0,1)\)`, cuando `\(n \to +\infty\)`.

Ejemplo. Sea `\(X_{1}, X_{2}, ... , X_{n}\)` una muestra aleatoria de una población con media `\(\mu\)` y variancia `\(\sigma^2\)`. Demuestre que `\(V = \frac{\overline{X}-\mu}{\frac{S}{\sqrt{n}}}\)` converge a una `\(N(0,1)\)` cuando `\(n \to +\infty\)`. 
	
Solución. Sea `\(U_n = \frac{\overline{X}-\mu}{\frac{\sigma}{\sqrt{n}}}\)`. Sabemos que por el Teorema del Límite Central `\(U_n\)` converge en distribución a una `\(N(0,1)\)` cuando `\(n \to +\infty\)`. 

Sea `\(W_n = \frac{S}{\sigma}\)`. Podemos demostrar que `\(S\)` es consistente para estimar `\(\sigma\)` para cualquier población, por lo que por el Definición 1.10, `\(\frac{S}{\sigma}\)` converge en probabilidad a 1 ("es consistente para estimar 1"). 
	
Entonces se cumple que `\(V = \frac{\overline{X}-\mu}{\frac{S}{\sqrt{n}}} \xrightarrow{\text{d}} N(0,1)\)` cuando `\(n \to +\infty\)`. 

---

# Suficiencia
	
Hasta el momento la selección de estimadores ha sido intuitiva, sin embargo en esta sección utilizaremos la propiedad de suficiencia para determinar estimadores a partir de ciertos estadísticos. Se dice que un estadístico es suficiente si **hace uso de toda la información de la muestra**. Ejemplos: `\(\overline{X}, S^{2}, X_{(n)}\)`.
	
&gt; Definición 1.11. Suficiencia minima. Si `\(X_{1}, X_{2}, ... , X_{n}\)` es una muestra aleatoria sobre una población con parámetro desconocido `\(\theta\)` y función de densidad/probabilidad `\(f_{X}(x|\theta)\)`. Se dice que un estadístico `\(U = g(X_{1}, X_{2}, ... , X_{n})\)` es **suficiente mínimo** para estimar `\(\theta\)` si la distribución condicional de `\(X_{1}, X_{2}, ... , X_{n}\)` dado `\(U=u\)` es independiente de `\(\theta\)`. 

---

# Suficiencia

De otra forma, se puede decir que un estadístico `\(U = T(X_{1}, X_{2}, ... , X_{n})\)`, de una muestra aleatoria `\(X_{1}, X_{2}, ... , X_{n}\)`, es suficiente mínimo para estimar `\(\theta\)` si no se puede encontrar otro estadístico que realice una mejor reducción de los datos que la que realiza `\(U\)`. Es decir, el estadístico suficiente mínimo logra **explicar toda la información del parámetro que se presenta en la muestra aleatoria**.
	
Ejemplo. Sea `\(X_{1}, X_{2}, ... , X_{n}\)` una muestra aleatoria tal que `\(X_{j} \sim Bernoulli(p)\)`. Pruebe que `\(U = \sum_{j=1}^{n} X_{j}\)` es suficiente para estimar `\(p\)`.
	
Solución. Sabemos que la función de probabilidad de una Bernoulli viene dada por la siguiente expresión:
	
`\(f_{X}(x|p) = \begin{cases} p^{x}\left(1-p\right)^{1-x} \quad si \quad x \in \left\lbrace 0,1\right\rbrace  \\ 0 \quad en \quad otros \quad casos \end{cases}\)`
	
También sabemos de antemano que la suma de Bernoulli es una Binomial, por lo que `\(U \sim Bin(n,p)\)`, por lo que tiene la siguiente función de probabilidad:
	
`\(f_{U}(u|p) = \begin{cases} \binom{n}{u}p^{u}\left(1-p\right)^{n-u} \quad si \quad u \in \left\lbrace0,1,2,...,n\right\rbrace \\ 0 \quad en \quad otros \quad casos \end{cases}\)`
	
---

# Suficiencia

Ahora tenemos que encontrar la función de probabilidad condicional de `\(X_{1}, X_{2}, ... , X_{n}\)` dado `\(U=u\)`, que por lo visto en cursos anteriores sabemos que es:
	
`\(f(x_{1}, ... , x_{n} | U = u) = \frac{f(x_{1}, ... , x_{n}, u)}{f_{U}(u)}\)`

En este caso el numerador es la función de probabilidad conjunta de `\(X_{1}, X_{2}, ... , X_{n}\)` y `\(U\)`, pero al estar este en términos de toda la muestra aleatoria entonces quedamos con solo la función de probabilidad conjunta de `\(X_{1}, X_{2}, ... , X_{n}\)`, `\(f(x_{1}, ... , x_{n})\)`. Recordemos que bajo independencia `\(f(x_{1}, ... , x_{n}) = \prod_{j=1}^{n}f_{X_{j}}(x_{j}|p)\)`. Esta función también lleva el nombre de **función de verosimilitud** y se denota como `\(\mathcal{L}(x_{1}, ... , x_{n}|p)\)` o también solo como `\(\mathcal{L}(p)\)`. 
	
`\(\mathcal{L}(x_{1}, ... , x_{n}|p) =  \prod_{j=1}^{n} p^{x_{j}}\left(1-p\right)^{1-x_{j}}= p^{\sum_{j=1}^{n}x_{j}}\left(1-p\right)^{n-\sum_{j=1}^{n}x_{j}} = p^{u}\left(1-p\right)^{n-u}\)`


---

# Suficiencia

Por lo tanto, `\(f(x_{1}, ... , x_{n} | U = u) = \frac{p^{u}\left(1-p\right)^{n-u}}{\binom{n}{u}p^{u}\left(1-p\right)^{n-u}} = \frac{1}{\binom{n}{u}}\)`
	
Como vemos, esta función condicional no depende de `\(p\)`, por lo que decimos que `\(U\)` es suficiente para estimar `\(p\)`. 

Ejemplo. Sea `\(X_{1}, X_{2}, ... , X_{n}\)` una muestra aleatoria de una población Uniforme en el intervalo `\((0,\theta)\)`. Demuestre que el máximo muestral es un estimador suficiente para `\(\theta\)`. 
	
Solución. Ya habiamos demostrado con anterioridad que para este caso `\(U = X_{(n)} \sim Potencial(n,\theta)\)`, por lo que conocemos su función de densidad:

`\(f_{U}(u) = \frac{nu^{n-1}}{\theta^n}\)`
	
Con esto podemos encontrar la función de densidad marginal:
	
`\(f(x_{1}, ... , x_{n} | U = u) = \frac{\theta^{-n}}{\frac{nu^{n-1}}{\theta^n}} = \frac{1}{nu^{n-1}}\)`
	
Como esta expresión no depende de `\(\theta\)` entonces decimos que el máximo muestral es suficiente para estimar `\(\theta\)`. 

---

# Técnicas para demostrar suficiencia
	
&gt; Teorema 1.26. Técnica de factorización. Si `\(U\)` es un estadístico definido sobre una muestra aleatoria `\(X_{1}, X_{2}, ... , X_{n}\)` de una población con parámetro desconocido `\(\theta\)` y `\(\mathcal{L}(X_{1}, X_{2}, ... , X_{n}|\theta) = \mathcal{L}(\theta)\)` es la función de verosimilitud entonces `\(U\)` es suficiente para `\(\theta\)` si y solo si existen funciones `\(g(u,\theta)\)` y `\(h(x_{1}, x_{2}, ... , x_{n})\)` tal que `\(\mathcal{L}(\theta) = g(u,\theta) \cdot h(x_{1}, x_{2}, ... , x_{n})\)` donde `\(g(\cdot)\)` depende de `\(X_{1}, X_{2}, ... , X_{n}\)` solo por medio de `\(U\)` y `\(h(\cdot)\)` no depende de `\(\theta\)`. 

Ejemplo. Sea `\(X_{1}, X_{2}, ... , X_{n}\)` una muestra aleatoria tal que `\(X_{j} \sim Bernoulli(p)\)`. Pruebe que `\(U = \sum_{j=1}^{n} X_{j}\)` es un estadístico suficiente mínimo para `\(p\)`. 

Solución. En el caso de una muestra aleatoria Bernoulli, su función de verosimilitud viene dada por
	
`\(\mathcal{L}(p) = \prod_{j=1}^{n} p^{x_{j}}\left(1-p\right)^{1-x_{j}} = p^{\sum x_{j}}\left(1-p\right)^{n-\sum x_{j}} = p^{u}\left(1-p\right)^{n-u}\)`
	
Si tomamos `\(g(u,p) = p^{u}\left(1-p\right)^{n-u}\)` y `\(h(x_{1}, x_{2}, ... , x_{n}) = 1\)` podemos ver que se cumple el teorema anterior por lo que queda demostrado que `\(U = \sum_{j=1}^{n} X_{j}\)` es un estadístico suficiente mínimo para `\(p\)`.
---

# Suficiencia

Ejemplo. Sea `\(X_{1}, X_{2}, ... , X_{n}\)` una muestra aleatoria de una población Poisson con media `\(\lambda\)`. Encuentre un estadístico suficiente mínimo para `\(\lambda\)`. 
	
Solución. Primero debemos encontrar la función de verosimilitud para una muestra aleatoria Poisson,
	
`\(\mathcal{L}(\lambda) = \prod_{j=1}^{n} \frac{\lambda^{x_{j}}e^{-\lambda}}{x_{j}!} = \frac{\lambda^{\sum x_{j}}e^{-n\lambda}}{\prod x_{j}! }\)`

Si tomamos `\(U = \sum_{j=1}^{n} X_{j}\)` entonces podemos observar que `\(g(u,\lambda) = \lambda^{u} e^{-n\lambda}\)` y `\(h(x_{1}, x_{2}, ... , x_{n}) = \frac{1}{\prod x_{j}! }\)` cumplen que su producto sea igual a la verosimilitud, por lo que `\(U = \sum_{j=1}^{n} X_{j}\)` es un estadístico suficiente mínimo. 

---

# Suficiencia

&gt; Teorema 1.27. Técnica de la familia exponencial. Si `\(X\)` es una variable aleatoria cuyo dominio no depende de un parámetro desconocido `\(\theta\)` y la función de densidad/probabilidad de X dado `\(\theta\)` pertenece a la familia exponencial, es decir que tiene la forma:
`\(f_{X}(x|\theta) = b(x)c(\theta)e^{-a(x)d(\theta)}, \qquad a(x) \neq 1\)`
entonces `\(U = \sum_{j=1}^{n} a(X_{j})\)` es un estadístico suficiente mínimo para estimar `\(\theta\)`. 

Prueba. Sea `\(X_{1}, X_{2}, ... , X_{n}\)` una muestra aleatoria tal que `\(X_{j}\)` pertenece a la familia exponencial. Entonces se cumple,
	
`\(\mathcal{L}(\theta) = \prod_{j=1}^{n} b(x_{j})c(\theta)e^{-a(x_{j})d(\theta)} = c(\theta)^{n} \prod b(x_{j}) e^{-d(\theta)\sum a(x_{j}) }\)`
	
Si tomamos como `\(U = \sum_{j=1}^{n} a(X_{j})\)`, `\(g(u,\theta) = c(\theta)^{n} e^{-d(\theta)u }\)` y `\(h(x_{1}, x_{2}, ... , x_{n}) = \prod b(x_{j})\)` entonces, por la técnica de factorización, se cumple que `\(U\)` es suficiente mínimo para `\(\theta\)`. 

---

# Suficiencia

Ejemplo. Encuentre el estadístico suficiente mínimo para el ejemplo anterior pero utilizando la técnica de la familia exponencial.
	
Solución. Lo primero consiste en demostrar que la función de probabilidad de una Poisson tiene la forma de la familia exponencial. Viendo la función de probabilidad de una Poisson, `\(f(x|\lambda) = \frac{\lambda^{x}e^{-\lambda}}{x!}\)`, esta pareciera no cumplir la forma de la familia exponencial, no obstante podemos realizar algunas operaciones algebraicas para alcanzar esa forma: 
	
`\(f(x|\lambda) = \frac{\lambda^{x}e^{-\lambda}}{x!} = e^{\ln\left(\frac{\lambda^{x}e^{-\lambda}}{x!}\right)}= e^{\ln(\lambda^{x}) + \ln(e^{-\lambda}) - \ln(x!)}\)` 
	
`\(= e^{x\ln(\lambda) - \lambda - \ln(x!)} = e^{-\lambda}e^{x\ln(\lambda)}\frac{1}{x!}\)`

---

# Suficiencia

Podemos ver que esta expresión tiene la forma de la familia exponencial con 

`\(\begin{matrix} a(x)=x &amp; c(\lambda )={ e }^{ -\lambda  } \\ b(x)=\frac { 1 }{ x! }  &amp; d(\lambda )=-\ln { (\lambda ) }  \end{matrix}\)`
	
Por lo tanto podemos concluir que `\(U = \sum_{j=1}^{n} X_{j}\)` es un estadístico suficiente mínimo para `\(\lambda\)`. 
	
Ejemplo. Sea `\(X_{1}, X_{2}, ... , X_{n}\)` una muestra aleatoria de una población con función de densidad:
	
`\(f_{X}(x) = \begin{cases}\frac{\alpha x^{\alpha-1}}{\theta} e^{-\frac{x^{\alpha}}{\theta}} \quad si \quad x &gt; 0 \\	0 \quad si \quad x \leq 0	\end{cases}\)`
	
Encuentre un estadístico suficiente mínimo para `\(\theta\)`. 

---

# Suficiencia

Solución. Podemos apreciar de la función de densidad anterior lo siguiente: 
	
`\(\begin{matrix} a(x) = x^{\alpha} &amp; c(\theta) = \frac{1}{\theta} \\ b(x) = \alpha x^{\alpha-1} &amp; d(\theta) = \frac{1}{\theta} \end{matrix}\)`
	
Por lo tanto, por la técnica de la familia exponencial, el estadístico `\(U = \sum_{j=1}^{n} X_{j}^{\alpha}\)` es suficiente mínimo para `\(\theta\)`. 
	
Supongamos que para este caso si quisieramos saber cuál sería un estadístico suficiente mínimo para `\(\alpha\)`. De las técnicas vistas hasta el momento no es posible obtener una respuesta, no obstante veremos posteriormente una estrategia para resolver este problema. 

---

### Ejercicios:

1. Una variable `\(X\)` se distribuye como una `\(\chi^2_{\kappa}\)`. Considere un estadístico `\(T\)` que usa la información contenida en una muestra aleatoria `\(X_1, ..., X_n\)`. Si `\(T(X) = 2 \bar{X} -1\)`, calcule el error cuadrático medio de `\(T(X)\)`, y defina si es o no un estimador consistente.

2. Sea `\(Y_1, ..., Y_n\)` una muestra independiente e idénticamente distribuida de una `\(N(\mu, \sigma^2)\)`. Entonces:

a. Si `\(\mu\)` es desconocido y `\(\sigma^2\)` es conocida, entonces muestre que `\(\bar{Y}\)` es suficiente para `\(\mu\)`.

b. Si `\(\mu\)` es conocido y `\(\sigma^2\)` es desconocida, entonces muestre que `\(\sum_{i=1}^{n} (Y_i - \mu)^2\)` es suficiente para `\(\sigma^2\)`. 

c. Si `\(\mu\)` y `\(\sigma^2\)` son desconocidas, muestre que `\(\sum_{i=1}^{n} Y_i\)` y `\(\sum_{i=1}^{n} Y_i^2\)` son conjuntamente suficientes para `\(\mu\)` y `\(\sigma^2\)`. 


---
class: center, middle

# ¿Qué discutimos hoy?

Propiedades de los estimadores: insesgado, eficiente. 

Próxima clase: Asignación del proyecto 1. Teorema de factorización. Familias de distribuciones. Estimadores insesgados de variancia mínima. 


Slides creadas via R package [**xaringan**](https://github.com/yihui/xaringan).

El chakra viene de [remark.js](https://remarkjs.com), [**knitr**](http://yihui.name/knitr), and [R Markdown](https://rmarkdown.rstudio.com).
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
